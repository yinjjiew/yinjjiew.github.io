<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content=""/><meta name="msvalidate.01" content=""/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>RLAnything | Yinjie Wang</title> <meta name="author" content="Yinjie Wang"/> <meta name="description" content="Forge Environment, Policy, and Reward Model in Completely Dynamic RL System"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/favicon_white.ico"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://yinjjiew.github.io/projects/rlanything/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Yinjie </span>Wang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">blogs</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h2 class="post-title">RLAnything</h2> <p class="post-description">Forge Environment, Policy, and Reward Model in Completely Dynamic RL System</p> </header> <article> <p>In real-world applications, an agent must interact with the environment iteratively, often producing long trajectories. In this setting, step-wise signals are especially important. Beyond high-quality reward modeling, task difficulty is known to affect an agent’s learning efficiency. We introduce <strong>RLAnything</strong>, a fully dynamic RL system that tightly couples the environment, policy, and reward model by having them provide feedback to one another, strengthening learning signals and improving overall performance. We conduct extensive experiments across GUI agent, LLM agent, and coding LLM settings. See details in our <a href="https://arxiv.org/abs/2509.06949" target="_blank" rel="noopener noreferrer">paper</a> and <a href="https://github.com/Gen-Verse/Open-AgentRL" target="_blank" rel="noopener noreferrer">repository</a>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rlanythingoverview-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rlanythingoverview-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rlanythingoverview-1400.webp"></source> <img src="/assets/img/rlanythingoverview.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h5 id="insights-overview">Insights Overview</h5> <ol> <li>The policy is trained with integrated feedback from outcome signals and step-wise from reward model, better than using outcome only.</li> <li>Reward model is jointly optimized via consistency feedback, which in turn further improves policy training.</li> <li>Our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience.</li> <li>Through extensive experiments, we demonstrate each added dynamic component consistently improves the overall system.</li> <li>Step-wise signals from optimized reward-model outperform outcome signals that rely on human labels.</li> </ol> <p><br> <br></p> <h5 id="integrated-feedback-for-policy">Integrated Feedback for Policy</h5> <p>Given a task \(q \in Q\) and policy \(\pi \), we obtain a trajectory \(\tau \sim \pi (\cdot \mid q) \) and its corresponding outcome \( O_{\tau} \in \{-1, 1 \} \). For the \(i\)-th step \(\tau_i \), we query reward model \(m\) independent times, yielding reasoning \( r_{\tau_i, j} \) and final score \(S_{\tau_i, j} \in \{-1, 1\} \), \(1 \le j \le m\). The the integrated feedback for \(\tau_i \) is</p> \[R_{\tau_i} = O_{\tau} + \sum_{j = 1}^m S_{\tau_i, j} / m,\] <p>where \( \lambda \) (1 by default) balances the trade-off between outcome supervision and step-wise supervision. Note that we compute advantages by standardizing rewards across trajectories at the same step index \(i\), i.e., over the set \( \{ R_{\tau_i} \mid \tau \sim \pi (\cdot \mid q) \} \). From the following figures, we find this integrated feedback is vital for long trajectory tasks.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rlanything_int_vs_out-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rlanything_int_vs_out-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rlanything_int_vs_out-1400.webp"></source> <img src="/assets/img/rlanything_int_vs_out.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><br> <br></p> <h5 id="consistency-feedback-for-reward-model">Consistency Feedback for Reward Model</h5> <p>We further strengthen the policy’s training signals by jointly improving the reward model. We set the consistency feedback for \( r_{\tau_i, j} \) as</p> \[R_{S_{\tau_i, j}} = R_{\tau_i} \cdot S_{\tau_i, j},\] <p>where the supervision is consisted of both outcome and self-consistency signals. We evaluate two aspects of reward modeling. First, we assess the ability to detect errors at the current step (we obtain step-wise ground-truth labels via majority voting from a much stronger reasoning model). Second, we assess its ability to predict the influence on final outcome; we use the episode outcome as the label for this evaluation. From the following figure, we kown the reward quality is improving and further improves policy training.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rlanything_rm_and_policy_improve_by_side-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rlanything_rm_and_policy_improve_by_side-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rlanything_rm_and_policy_improve_by_side-1400.webp"></source> <img src="/assets/img/rlanything_rm_and_policy_improve_by_side.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><br> <br></p> <h5 id="balancing-task-difficulty-benefits-both-the-reward-and-policy-models">Balancing Task Difficulty Benefits Both the Reward and Policy Models</h5> <p>We know balancing task difficulty can improve policy training. We present two simple theoretical results showing that, under our reward system, this approach also improves reward-model training. Theorem 1 transforms the objective of predicting each step’s influence on the final outcome, into two terms (\( p_{+}\) and \( p_{-}\)) that can be estimated from our rollout data.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rlanythingthm1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rlanythingthm1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rlanythingthm1-1400.webp"></source> <img src="/assets/img/rlanythingthm1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>This theorem highlights the necessity of balancing the importance-sampling ratio between \( p_{+}\) and \( p_{-}\) to prevent bias in reward-model optimization. We then use the following Theorem 2 to show that our actual optimization target reduces to estimator of \(\mu \) and that task-difficulty balancing guarantees a well-behaved importance-sampling ratio.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rlanythingthm2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rlanythingthm2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rlanythingthm2-1400.webp"></source> <img src="/assets/img/rlanythingthm2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><br> <br></p> <h5 id="adapting-environment-tasks-by-critic-feedback-from-both-reward-and-policy-models">Adapting Environment Tasks by critic feedback from both reward and policy models</h5> <p>Building on the above insights, we now propose a targeted and automated approach to task adaptation. If the policy’s accuracy on \(q\) falls outside \((\alpha_{\text{low}}, \alpha_{\text{high}})\), we ask a language model to modify \(q\) into \(q’\) using a summarized critic derived from \(\{r_{\tau_i,j} \mid \tau \sim \pi(\cdot \mid q), \forall i,j\}\), i.e., the reward model’s previous outputs. The quality of the new task \(q’\) is ensured by our acceptance mechanism. If the goal is to make \(q\) harder, we replace it with \(q’\) only if \(\alpha_{\text{low}} &lt; acc(q’) &lt; acc(q)\). If the goal is to make \(q\) easier, we replace it with \(q’\) only if \(acc(q) &lt; acc(q’) &lt; \alpha_{\text{high}}\). By default, we set \(\alpha_{\text{low}} = 0.2\) and \(\alpha_{\text{high}} = 0.8\). Note that the correctness of \(q’\) is also guaranteed, since it must yield at least one successful rollout (i.e., \(acc(q’) &gt; 0\)).</p> <p><br> <br></p> <h5 id="each-added-dynamic-component-consistently-improves-the-overall-system">Each Added Dynamic Component Consistently Improves The Overall System</h5> <p>From the table and figures below, we show that each added dynamic component consistently improves the overall system and enhances generalizability.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rlanythingmaintable-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rlanythingmaintable-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rlanythingmaintable-1400.webp"></source> <img src="/assets/img/rlanythingmaintable.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rlanything_alfworld_osworld_curve_side_by_side-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rlanything_alfworld_osworld_curve_side_by_side-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rlanything_alfworld_osworld_curve_side_by_side-1400.webp"></source> <img src="/assets/img/rlanything_alfworld_osworld_curve_side_by_side.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><br> <br></p> <h5 id="step-wise-signals-from-an-optimized-reward-model-outperform-outcome-signals-that-rely-on-human-labels">Step-wise Signals from an Optimized Reward Model Outperform Outcome Signals that Rely on Human Labels</h5> <p>We compare two training signals: step-wise signals from the optimized reward model and outcome signals. We find that the optimized reward model provides substantially stronger supervision. In real-world applications such as GUI agents, outcome rewards often require humans to create verifier files, which limits scalability. Motivated by these results, self-exploring agents in large-scale environments such as computers become more promising.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rlanything_optimized_rm-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rlanything_optimized_rm-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rlanything_optimized_rm-1400.webp"></source> <img src="/assets/img/rlanything_optimized_rm.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><br> <br></p> <h5 id="adaptation-examples">Adaptation Examples</h5> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rlanythingexample-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rlanythingexample-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rlanythingexample-1400.webp"></source> <img src="/assets/img/rlanythingexample.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><br> <br></p> <h5 id="linear-scaling-of-environment-tasks">Linear Scaling of Environment Tasks</h5> <p>We find that the number of accepted new environment tasks grows approximately linearly, demonstrating the potential for environment scaling. In our text-agent (AlfWorld) and coding settings, new tasks are generated fully automatically by language models. In the GUI-agent setting (OSWorld), we pre-create additional verifier files. Although task modification and hint generation are still automated in the GUI setting, the need to pre-create verifiers can limit automation and scalability, making this an important direction for future work.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rlanythinglinearenv-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rlanythinglinearenv-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rlanythinglinearenv-1400.webp"></source> <img src="/assets/img/rlanythinglinearenv.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2026 Yinjie Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Last updated: February 01, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script>window.MathJax={tex:{tags:"ams",inlineMath:[["\\(","\\)"]],displayMath:[["\\[","\\]"],["$$","$$"]],processEscapes:!0},chtml:{scale:.9,linebreaks:{automatic:!0,width:"container"}}};</script> <script defer id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> </body> </html>