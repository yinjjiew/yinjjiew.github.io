<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content=""/><meta name="msvalidate.01" content=""/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>dllm-rl | Yinjie Wang</title> <meta name="author" content="Yinjie Wang"/> <meta name="description" content="Reinforcement Learning for Diffusion Language Models"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/favicon_white.ico"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://yinjjiew.github.io/projects/dllmrl/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Yinjie </span>Wang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">blogs</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h2 class="post-title">dllm-rl</h2> <p class="post-description">Reinforcement Learning for Diffusion Language Models</p> </header> <article> <p>Reinforcement learning is an important methodology to improve language models. We propose a comprehensive RL framework for discrete diffusion language models (see <a href="https://arxiv.org/abs/2509.06949" target="_blank" rel="noopener noreferrer">paper</a>), paired with post-training <a href="https://github.com/Gen-Verse/dLLM-RL" target="_blank" rel="noopener noreferrer">repo</a>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/dllmrlposterstart-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/dllmrlposterstart-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/dllmrlposterstart-1400.webp"></source> <img src="/assets/img/dllmrlposterstart.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h5 id="insights-overview">Insights Overview</h5> <ol> <li>Prior methods omit <strong>trajectory awareness</strong>, leading to a mismatch between optimization and inference. In practice, diffusion language models employ confidence-driven sampling, which differs from the purely random sampling used in image diffusion.</li> <li>Building on this, we propose <a href="https://arxiv.org/abs/2509.06949" target="_blank" rel="noopener noreferrer">TraceRL</a>, a trajectory-aware RL method applicable to both full- and block-attention DLMs. With trajectory shrinkage and sliced block training, we accelerate training.</li> <li>We introduce a diffusion value model to stabilize training via <strong>token-wise variance-reduction</strong> baselines.</li> <li>We open-source a comprehensive post-training repo <a href="https://github.com/Gen-Verse/dLLM-RL" target="_blank" rel="noopener noreferrer">dLLM-RL</a> for current open-source diffusion language models, including multiple SFT and RL methods, across math, coding, multimodal and RLHF settings, support process reward model and single/multiple nodes training.</li> <li>We also implement a 7B <a href="https://huggingface.co/collections/Gen-Verse/trado-series" target="_blank" rel="noopener noreferrer">long-CoT block diffusion model</a>. Based on our findings, traditional full-attention DLMs cannot handle long-context settings well; block diffusion is likely to become the dominant approach.</li> </ol> <h5 id="trajectory-matters-for-dlm-post-training">Trajectory matters for DLM post-training</h5> <p>This is a simple demonstration experiment comparing different SFT methods for DLMs. Fully random masking is the commonly used SFT approach for full-attention DLMs. However, it overlooks the inherent logical structure of language, especially in reasoning tasks. Semi-AR SFT is the typical training method for block diffusion models: it applies random masks within each block while preserving block-wise causality. But applying Semi-AR training to full-attention models is slow because it requires slicing the data. We also explore trace-wise SFT, where a DLM collects its own confidence-driven trajectories and uses them for self-SFT. We find that, under comparable training time, fully random &lt; Semi-AR &lt; trace-wise.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/demonexp-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/demonexp-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/demonexp-1400.webp"></source> <img src="/assets/img/demonexp.png" class="img-fluid rounded z-depth-1 w-60" width="auto" height="auto" title="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h5 id="tracerl">TraceRL</h5> <p>Given the above insights, we propose <strong>TraceRL</strong>. For each generated response $\tau_i$ given the task $Q$, we represent it as a trajectory</p> <table> <tbody> <tr> <td>$\tau_i \triangleq \tau_i^{(1)} \cup \cdots \cup \tau_i^{(</td> <td>\tau_i</td> <td>)},$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>where $</td> <td>\tau_i</td> <td>$ is the number of decoding steps, and $\tau_i^{(t)}$ is the set of tokens decoded during the $t$-th step. TraceRL rewards or penalizes the sampling trajectory under policy $\pi_\theta$ based on the verifiable reward $r_i$ assigned to $\tau_i$. When using RLVR, $r_i$ is equivalent to the verifiable outcome.</td> </tr> </tbody> </table> <p>To accelerate training, we aggregate every $s$ neighboring steps. Specifically, we compress $\tau_i$ into</p> <table> <tbody> <tr> <td>$\tau_i^{s} \triangleq \tau_i^{s(1)} \cup \cdots \cup \tau_i^{s!\left(</td> <td>\tau_i^{s}</td> <td>\right)},$</td> </tr> </tbody> </table> <p>where</p> <p>$\tau_i^{s(k)} \triangleq \bigcup_{j=s(k-1)+1}^{\min(sk,\,|\tau_i|)} \tau_i^{(j)}, \qquad |\tau_i^{s}| = \left\lceil \frac{|\tau_i|}{s} \right\rceil .$</p> <p>The policy loss is</p> <p>$J_{\mathrm{policy}}(\theta_p) = \mathbb{E}<em>{Q \sim D</em>{\mathrm{task}},\, {\tau_i}<em>{i=1}^{G} \sim \pi</em>{\mathrm{old}}(\cdot \mid Q)} \Bigg( \sum_{i=1}^{G} \sum_{t=1}^{|\tau_i^{s}|} \frac{1}{|\tau_i^{s(t)}|} \sum_{o_k \in \tau_i^{s(t)}} C_{\epsilon}!\left( \frac{\pi_{\theta_p}!\left(o_k \mid \tau_i^{s}(1{:}(t-1))\right)} {\pi_{\mathrm{old}}!\left(o_k \mid \tau_i^{s}(1{:}(t-1))\right)}, A_i \right) \Bigg) - \beta\,\mathrm{KL}.$</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Yinjie Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Last updated: November 30, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> </body> </html>